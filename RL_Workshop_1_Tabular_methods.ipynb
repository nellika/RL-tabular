{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9UPHRWEyDLaSyuLkl5Uwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nellika/RL-tabular/blob/main/RL_Workshop_1_Tabular_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GZeimZKWZcbt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "IlnB8Lm1aY6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the state space and action space\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ],
      "metadata": {
        "id": "GMm0E6kCab_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Q\n",
        "!mkdir SARSA"
      ],
      "metadata": {
        "id": "sjeJbs7ceUzh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning"
      ],
      "metadata": {
        "id": "MmWW_LjFiWCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn(state, state2, reward, action, lr_rate):\n",
        "  predict = Q[state, action]\n",
        "  target = reward + gamma * np.max(Q[state2, :])\n",
        "  Q[state, action] = Q[state, action] + lr_rate * (target - predict)"
      ],
      "metadata": {
        "id": "8B0yW9vnayfd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state):\n",
        "  action = np.argmax(Q[state,:])\n",
        "  return action\n",
        "\n",
        "def choose_action(state, episode):\n",
        "  action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
        "  return action"
      ],
      "metadata": {
        "id": "BJODolQUcVRD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "max_steps = 50\n",
        "num_episodes = 5000\n",
        "G = collections.deque(maxlen=50)\n",
        "episode_lengths = []\n",
        "Q_tables = []\n",
        "\n",
        "gamma = 0.99\n",
        "lr_rate = 0.85\n",
        "\n",
        "frames = []\n",
        "\n",
        "for episode in tqdm(range(num_episodes), desc =\"Training: \", leave=True):\n",
        "  state = env.reset()\n",
        "\n",
        "  episode_length = 0\n",
        "  reward_sum = 0\n",
        "\n",
        "  if episode % 500 == 0:\n",
        "      frames = []\n",
        "      frames.append(np.array(env.render())[0])\n",
        "      Q_tables.append(Q.copy())\n",
        "\n",
        "  done = False\n",
        "  while not done or episode_length < max_steps:\n",
        "    env.render()\n",
        "    action = choose_action(state, episode)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    learn(state, new_state, reward, action, lr_rate)\n",
        "    state = new_state\n",
        "\n",
        "    if episode % 500 == 0: frames.append(np.array(env.render())[0])\n",
        "    reward_sum += reward\n",
        "    episode_length += 1\n",
        "\n",
        "  episode_lengths.append(episode_length)\n",
        "  G.append(reward_sum)\n",
        "\n",
        "  if episode % 500 == 0:\n",
        "      tqdm.write(\"Success rate: \" + str(np.mean(G)))\n",
        "      imageio.mimwrite(f\"Q/episode_{episode}.gif\", frames, fps=8)"
      ],
      "metadata": {
        "id": "acGidI_aa7-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,len(Q_tables))\n",
        "for i in range(len(Q_tables)):\n",
        "  ax = axs[i]\n",
        "  max_values = np.array(Q_tables[i].max(axis=1)).reshape((4, 4))\n",
        "  im = ax.imshow(max_values, cmap='viridis')\n",
        "  # cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hz3rAbg4jWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,len(Q_tables))\n",
        "for i in range(len(Q_tables)):\n",
        "  ax = axs[i]\n",
        "  max_values = np.argmax(Q_tables[i], axis=1).reshape((4, 4))\n",
        "  im = ax.imshow(max_values, cmap='viridis')\n",
        "  # cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ag5GDgJonKXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA"
      ],
      "metadata": {
        "id": "xQHiQUVkoiQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn(state, new_state, reward, action, new_action):#SARSA\n",
        "  predict = Q[state, action]\n",
        "  target = reward + gamma * Q[new_state, new_action]\n",
        "  Q[state, action] = Q[state, action] + lr_rate * (target - predict)"
      ],
      "metadata": {
        "id": "8bhFYDVeonKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state):\n",
        "  action = np.argmax(Q[state,:])\n",
        "  return action\n",
        "\n",
        "def choose_action(state, episode):\n",
        "  action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
        "  return action"
      ],
      "metadata": {
        "id": "uAu-FBUqoqem"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "max_steps = 50\n",
        "num_episodes = 5000\n",
        "G = collections.deque(maxlen=50)\n",
        "episode_lengths = []\n",
        "Q_tables = []\n",
        "\n",
        "gamma = 0.99\n",
        "lr_rate = 0.85\n",
        "\n",
        "frames = []\n",
        "\n",
        "for episode in tqdm(range(num_episodes), desc =\"Training: \", leave=True):\n",
        "  state = env.reset()\n",
        "\n",
        "  episode_length = 0\n",
        "  reward_sum = 0\n",
        "\n",
        "  if episode % 500 == 0:\n",
        "      frames = []\n",
        "      frames.append(np.array(env.render())[0])\n",
        "      Q_tables.append(Q.copy())\n",
        "\n",
        "  done = False\n",
        "  while not done or episode_length < max_steps:\n",
        "    env.render()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    new_action = choose_action(new_state, episode)\n",
        "    learn(state, new_state, reward, action, new_action)\n",
        "    state = new_state\n",
        "    action = new_action\n",
        "\n",
        "    if episode % 500 == 0: frames.append(np.array(env.render())[0])\n",
        "    reward_sum += reward\n",
        "    episode_length += 1\n",
        "\n",
        "  episode_lengths.append(episode_length)\n",
        "  G.append(reward_sum)\n",
        "\n",
        "  if episode % 500 == 0:\n",
        "      tqdm.write(\"Success rate: \" + str(np.mean(G)))\n",
        "      imageio.mimwrite(f\"SARSA/episode_{episode}.gif\", frames, fps=8)"
      ],
      "metadata": {
        "id": "XbZ2dgx5ossQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,len(Q_tables))\n",
        "for i in range(len(Q_tables)):\n",
        "  ax = axs[i]\n",
        "  max_values = np.array(Q_tables[i].max(axis=1)).reshape((4, 4))\n",
        "  im = ax.imshow(max_values, cmap='viridis')\n",
        "  # cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ayvnpVbrqpAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,len(Q_tables))\n",
        "for i in range(len(Q_tables)):\n",
        "  ax = axs[i]\n",
        "  max_values = np.argmax(Q_tables[i], axis=1).reshape((4, 4))\n",
        "  im = ax.imshow(max_values, cmap='viridis')\n",
        "  # cbar = ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BnBvgHAoqsBZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}